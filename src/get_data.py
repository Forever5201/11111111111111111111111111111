# get_data.py

import os
import json
import pickle
import pandas as pd
from datetime import datetime, timezone
from dotenv import load_dotenv
from logger import setup_logger
from config_loader import ConfigLoader
# from account_fetcher import AccountFetcher  # æš‚æ—¶æ³¨é‡Šï¼Œæµ‹è¯•æ—¶ä¸éœ€è¦è´¦æˆ·ä¿¡æ¯
from data_fetcher import DataFetcher
from technical_indicator import TechnicalIndicator

logger = setup_logger()

def select_indicator_params(config, timeframe):
    """
    æ ¹æ®timeframeé€‰æ‹©å¯¹åº”çš„æŒ‡æ ‡å‚æ•°
    :param config: é…ç½®ä¿¡æ¯
    :param timeframe: æ—¶é—´å‘¨æœŸ
    :return: å¯¹åº”çš„æŒ‡æ ‡å‚æ•°
    """
    for category, intervals in config.get('timeframes', {}).items():
        if timeframe in intervals:
            return config['indicators'].get(category, config['indicators']['midterm'])
    return config['indicators']['midterm']

def fetch_and_process_kline(data_fetcher, symbol, timeframe, config, is_mark_price=False):
    """
    è·å–å’Œå¤„ç†Kçº¿æ•°æ®ï¼ŒåŒ…æ‹¬è·å–å†å²Kçº¿å’Œå½“å‰æœªå®Œç»“Kçº¿
    """
    try:
        # è·å–è¯¥æ—¶é—´å‘¨æœŸçš„é…ç½®
        kline_config = config.get('kline_config', {}).get(timeframe, {})
        fetch_count = kline_config.get('fetch_count', 100)  # é»˜è®¤è·å–100æ¡
        output_count = kline_config.get('output_count', 50) # é»˜è®¤è¾“å‡º50æ¡
        
        logger.info(f"Fetching K-line data for {timeframe}...")
        
        # è·å–å†å²Kçº¿æ•°æ®
        kline_df = data_fetcher.fetch_kline_data(
            instrument_id=symbol,
            bar=timeframe,
            is_mark_price=is_mark_price,
            limit=fetch_count
        )
        
        if kline_df.empty:
            logger.warning(f"No K-line data for {timeframe}.")
            return pd.DataFrame()

        # è·å–å½“å‰æœªå®Œç»“Kçº¿
        current_kline_df = data_fetcher.get_current_kline(symbol, timeframe)
        
        # å¦‚æœæœ‰æœªå®Œç»“Kçº¿ï¼Œæ·»åŠ åˆ°æ•°æ®é›†
        if not current_kline_df.empty:
            kline_df = pd.concat([kline_df, current_kline_df])

        # ç¡®ä¿æŒ‰æ—¶é—´æ’åºå¹¶é‡ç½®ç´¢å¼•
        kline_df = kline_df.sort_values('timestamp', ascending=False)
        kline_df = kline_df.head(fetch_count).sort_values('timestamp')
        kline_df = kline_df.reset_index(drop=True)

        # é€‰æ‹©æŒ‡æ ‡å‚æ•°å¹¶è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        indicator_params = select_indicator_params(config, timeframe)
        
        if 'volume' not in kline_df.columns:
            kline_df['volume'] = 0

        logger.info(f"Calculating indicators for {timeframe}...")
        indicator_calculator = TechnicalIndicator(params=indicator_params)
        kline_df = indicator_calculator.calculate_all(kline_df, category=timeframe)

        # åªä¿ç•™æœ€è¿‘çš„æŒ‡å®šæ•°é‡çš„Kçº¿
        if len(kline_df) > output_count:
            kline_df = kline_df.tail(output_count)

        # è®¾ç½®Kçº¿çŠ¶æ€
        kline_df["is_closed"] = True
        if not current_kline_df.empty:
            kline_df.iloc[-1, kline_df.columns.get_loc("is_closed")] = False

        return kline_df

    except Exception as e:
        logger.error(f"Error processing K-line data for {timeframe}: {e}")
        return pd.DataFrame()

def convert_to_kronos_format(df, symbol_name):
    """
    å°†OKX Kçº¿æ•°æ®è½¬æ¢ä¸ºKronoså¾®è°ƒéœ€è¦çš„æ ¼å¼
    
    Args:
        df: OKX Kçº¿æ•°æ®DataFrame
        symbol_name: äº¤æ˜“å¯¹æ ‡è¯†ç¬¦
        
    Returns:
        è½¬æ¢åçš„Kronosæ ¼å¼DataFrame
    """
    if df.empty:
        logger.warning(f"Empty dataframe for {symbol_name}")
        return pd.DataFrame()
    
    try:
        # åˆ›å»ºKronosæ ¼å¼çš„DataFrame
        kronos_df = pd.DataFrame()
        
        # å­—æ®µæ˜ å°„ï¼šOKX -> Kronos
        field_mapping = {
            'open': 'open',
            'high': 'high', 
            'low': 'low',
            'close': 'close',
            'volume': 'vol',              # OKX volume -> Kronos vol
            'currency_volume': 'amt'      # OKX currency_volume -> Kronos amt
        }
        
        # æ˜ å°„å­—æ®µ
        for okx_field, kronos_field in field_mapping.items():
            if okx_field in df.columns:
                kronos_df[kronos_field] = pd.to_numeric(df[okx_field], errors='coerce')
            else:
                logger.warning(f"Field {okx_field} not found in data for {symbol_name}")
                kronos_df[kronos_field] = 0.0
        
        # å¤„ç†æ—¶é—´ç´¢å¼• - è½¬æ¢ä¸ºdatetimeå¹¶è®¾ä¸ºç´¢å¼•
        if 'time_iso' in df.columns:
            kronos_df.index = pd.to_datetime(df['time_iso'])
        elif 'timestamp' in df.columns:
            # å‡è®¾timestampæ˜¯ç§’æˆ–æ¯«ç§’æ—¶é—´æˆ³
            if df['timestamp'].iloc[0] > 1e10:  # æ¯«ç§’æ—¶é—´æˆ³
                kronos_df.index = pd.to_datetime(df['timestamp'], unit='ms')
            else:  # ç§’æ—¶é—´æˆ³
                kronos_df.index = pd.to_datetime(df['timestamp'], unit='s')
        else:
            logger.error(f"No timestamp field found for {symbol_name}")
            return pd.DataFrame()
        
        # ç¡®ä¿ç´¢å¼•åç§°ä¸ºdatetimeï¼ˆKronosè¦æ±‚ï¼‰
        kronos_df.index.name = 'datetime'
        
        # å¡«å……NaNå€¼
        kronos_df = kronos_df.fillna(0.0)
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        for col in ['open', 'high', 'low', 'close', 'vol', 'amt']:
            kronos_df[col] = kronos_df[col].astype(float)
        
        # æŒ‰æ—¶é—´æ’åº
        kronos_df = kronos_df.sort_index()
        
        logger.info(f"Converted {len(kronos_df)} records for {symbol_name}")
        return kronos_df
        
    except Exception as e:
        logger.error(f"Error converting {symbol_name} to Kronos format: {e}")
        return pd.DataFrame()

def fetch_historical_data_for_kronos(data_fetcher, symbol, timeframes, records_per_timeframe=1000):
    """
    è·å–å†å²æ•°æ®ç”¨äºKronoså¾®è°ƒ
    
    Args:
        data_fetcher: DataFetcherå®ä¾‹
        symbol: äº¤æ˜“å¯¹
        timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨
        records_per_timeframe: æ¯ä¸ªæ—¶é—´æ¡†æ¶è·å–çš„è®°å½•æ•°
        
    Returns:
        Dict[symbol_timeframe, DataFrame]: Kronosæ ¼å¼çš„æ•°æ®å­—å…¸
    """
    kronos_data = {}
    
    for timeframe in timeframes:
        try:
            logger.info(f"Fetching {records_per_timeframe} records for {symbol} {timeframe}")
            
            # è·å–å†å²æ•°æ®
            df = data_fetcher.fetch_kline_data(
                instrument_id=symbol,
                bar=timeframe,
                is_mark_price=False,
                limit=records_per_timeframe
            )
            
            if not df.empty:
                # è½¬æ¢ä¸ºKronosæ ¼å¼
                symbol_key = f"{symbol}_{timeframe}"
                kronos_df = convert_to_kronos_format(df, symbol_key)
                
                if not kronos_df.empty:
                    kronos_data[symbol_key] = kronos_df
                    logger.info(f"Successfully processed {len(kronos_df)} records for {symbol_key}")
                else:
                    logger.warning(f"Failed to convert data for {symbol_key}")
            else:
                logger.warning(f"No data fetched for {symbol} {timeframe}")
                
        except Exception as e:
            logger.error(f"Error processing {symbol} {timeframe}: {e}")
    
    return kronos_data

def split_data_for_kronos_training(data_dict, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """
    æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    
    Args:
        data_dict: åŸå§‹æ•°æ®å­—å…¸
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        
    Returns:
        (train_data, val_data, test_data) ä¸‰ä¸ªæ•°æ®å­—å…¸
    """
    if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
        logger.error("Data split ratios must sum to 1.0")
        return {}, {}, {}
    
    train_data = {}
    val_data = {}
    test_data = {}
    
    for symbol, df in data_dict.items():
        if df.empty:
            continue
            
        n_total = len(df)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)
        
        # æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²ï¼ˆæ—©æœŸ->è®­ç»ƒï¼Œä¸­æœŸ->éªŒè¯ï¼Œæœ€æ–°->æµ‹è¯•ï¼‰
        train_data[symbol] = df.iloc[:n_train].copy()
        val_data[symbol] = df.iloc[n_train:n_train+n_val].copy()
        test_data[symbol] = df.iloc[n_train+n_val:].copy()
        
        logger.info(f"Split {symbol}: train={len(train_data[symbol])}, "
                   f"val={len(val_data[symbol])}, test={len(test_data[symbol])}")
    
    return train_data, val_data, test_data

def save_kronos_datasets(train_data, val_data, test_data, output_dir="data/kronos_datasets"):
    """
    ä¿å­˜Kronosæ ¼å¼çš„æ•°æ®é›†ä¸ºpickleæ–‡ä»¶
    
    Args:
        train_data: è®­ç»ƒæ•°æ®å­—å…¸
        val_data: éªŒè¯æ•°æ®å­—å…¸
        test_data: æµ‹è¯•æ•°æ®å­—å…¸
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ä¸‰ä¸ªæ•°æ®é›†
        datasets = {
            'train_data.pkl': train_data,
            'val_data.pkl': val_data,
            'test_data.pkl': test_data
        }
        
        for filename, data in datasets.items():
            filepath = os.path.join(output_dir, filename)
            with open(filepath, 'wb') as f:
                pickle.dump(data, f)
            logger.info(f"Saved {filename} with {len(data)} symbols")
        
        # ä¿å­˜æ•°æ®æ‘˜è¦
        summary = {
            'train_symbols': len(train_data),
            'val_symbols': len(val_data), 
            'test_symbols': len(test_data),
            'total_train_records': sum(len(df) for df in train_data.values()),
            'total_val_records': sum(len(df) for df in val_data.values()),
            'total_test_records': sum(len(df) for df in test_data.values()),
            'created_at': datetime.now().isoformat(),
            'symbols': list(train_data.keys())
        }
        
        summary_path = os.path.join(output_dir, 'dataset_summary.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Successfully saved Kronos datasets to {output_dir}")
        logger.info(f"Summary: {summary}")
        return True
        
    except Exception as e:
        logger.error(f"Error saving Kronos datasets: {e}")
        return False

def prepare_kronos_training_data():
    """
    ä¸“é—¨ä¸ºKronoså¾®è°ƒå‡†å¤‡æ•°æ®çš„å‡½æ•°
    """
    logger.info("ğŸ¯ Starting Kronos training data preparation...")
    
    # åŠ è½½é…ç½®
    try:
        config_loader = ConfigLoader("../config/config.yaml")
        config = config_loader.load_config()
        logger.info("Configuration loaded.")
    except Exception as e:
        logger.error(f"Error loading configuration: {e}")
        return False

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    api_key = os.getenv("OKX_API_KEY")
    secret_key = os.getenv("OKX_API_SECRET")
    passphrase = os.getenv("OKX_API_PASSPHRASE")
    if not all([api_key, secret_key, passphrase]):
        logger.error("API credentials missing.")
        return False

    # åˆå§‹åŒ–æ•°æ®è·å–å™¨
    try:
        data_fetcher = DataFetcher(api_key, secret_key, passphrase)
        logger.info("DataFetcher initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing DataFetcher: {e}")
        return False

    # å®šä¹‰Kronoså¾®è°ƒçš„æ—¶é—´æ¡†æ¶å’Œå‚æ•°
    symbol = "BTC-USD-SWAP"
    timeframes = ["1H", "4H", "1D"]  # é€‚åˆKronosçš„æ—¶é—´æ¡†æ¶
    records_per_timeframe = 2000  # æ¯ä¸ªæ—¶é—´æ¡†æ¶è·å–2000æ¡è®°å½•
    
    logger.info(f"ğŸ“Š Fetching data for {symbol}")
    logger.info(f"â±ï¸ Timeframes: {timeframes}")
    logger.info(f"ğŸ“ˆ Records per timeframe: {records_per_timeframe}")
    
    # è·å–å†å²æ•°æ®
    kronos_data = fetch_historical_data_for_kronos(
        data_fetcher, symbol, timeframes, records_per_timeframe
    )
    
    if not kronos_data:
        logger.error("âŒ No data fetched for Kronos training")
        return False
    
    logger.info(f"âœ… Fetched data for {len(kronos_data)} timeframe combinations")
    
    # åˆ†å‰²æ•°æ®é›†
    logger.info("ğŸ“Š Splitting datasets...")
    train_data, val_data, test_data = split_data_for_kronos_training(kronos_data)
    
    # ä¿å­˜æ•°æ®é›†
    logger.info("ğŸ’¾ Saving Kronos datasets...")
    success = save_kronos_datasets(train_data, val_data, test_data)
    
    if success:
        logger.info("ğŸ‰ Kronos training data preparation completed!")
        logger.info("ğŸ“ Data saved to: data/kronos_datasets/")
        logger.info("ğŸ“„ Files created:")
        logger.info("   - train_data.pkl")
        logger.info("   - val_data.pkl") 
        logger.info("   - test_data.pkl")
        logger.info("   - dataset_summary.json")
        logger.info("ğŸš€ Ready for Kronos fine-tuning!")
        return True
    else:
        logger.error("âŒ Failed to save Kronos datasets")
        return False

def main():
    logger.info("Starting the data fetching process...")
    
    # åŠ è½½é…ç½®
    try:
        config_loader = ConfigLoader("../config/config.yaml")
        config = config_loader.load_config()
        logger.info("Configuration loaded.")
    except Exception as e:
        logger.error(f"Error loading configuration: {e}")
        return

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    api_key = os.getenv("OKX_API_KEY")
    secret_key = os.getenv("OKX_API_SECRET")
    passphrase = os.getenv("OKX_API_PASSPHRASE")
    if not all([api_key, secret_key, passphrase]):
        logger.error("API credentials missing.")
        return

    # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
    try:
        account_fetcher = AccountFetcher(api_key, secret_key, passphrase)
        data_fetcher = DataFetcher(api_key, secret_key, passphrase)
    except Exception as e:
        logger.error(f"Error initializing modules: {e}")
        return

    # è·å–è´¦æˆ·è¯¦ç»†ä¿¡æ¯
    logger.info("Fetching account data...")
    account_info = {"balance": 0, "positions": []}
    try:
        acct_bal = account_fetcher.get_balance()
        account_info = {
            "balance": acct_bal.get("balance", 0),
            "available_balance": acct_bal.get("available_balance", 0),
            "margin_ratio": acct_bal.get("margin_ratio", 0),
            "margin_frozen": acct_bal.get("margin_frozen", 0),
            "total_equity": acct_bal.get("total_equity", 0),
            "unrealized_pnl": acct_bal.get("unrealized_pnl", 0),
            "positions": account_fetcher.get_detailed_positions()
        }
    except Exception as e:
        logger.error(f"Error fetching account data: {e}")

    # è·å–æ‰€æœ‰éœ€è¦å¤„ç†çš„æ—¶é—´å‘¨æœŸ
    all_timeframes = []
    for category, tfs in config.get('timeframes', {}).items():
        all_timeframes.extend(tfs)

    symbol = "BTC-USD-SWAP"  # å¯ä»configæˆ–envä¸­åŠ è½½

    # å¤„ç†Kçº¿æ•°æ®
    timeframes_data = {}
    logger.info("Fetching actual price timeframes data...")
    for tf in all_timeframes:
        df = fetch_and_process_kline(data_fetcher, symbol, tf, config, is_mark_price=False)
        if not df.empty:
            timeframes_data[tf] = {
                "type": "actual_price",
                "indicators_params": select_indicator_params(config, tf),
                "data": df.to_dict(orient="records")
            }

    # è·å–å½“å‰å¸‚åœºæ•°æ®
    current_market_data = {}
    try:
        # è·å–å¸‚åœºåŸºç¡€æ•°æ®
        market_data = data_fetcher.fetch_ticker(symbol)
        # è·å–èµ„é‡‘è´¹ç‡æ•°æ®
        funding_data = data_fetcher.fetch_funding_rate(symbol)
        
        current_market_data = {
            "last_price": market_data.get("last_price", 0),
            "best_bid": market_data.get("best_bid", 0),
            "best_ask": market_data.get("best_ask", 0),
            "24h_high": market_data.get("24h_high", 0),
            "24h_low": market_data.get("24h_low", 0),
            "24h_volume": market_data.get("24h_volume", 0),
            "24h_turnover": market_data.get("24h_turnover", 0),
            "open_interest": market_data.get("open_interest", 0),
            "funding_rate": funding_data.get("funding_rate", 0),
            "next_funding_time": funding_data.get("next_funding_time", ""),
            "estimated_rate": funding_data.get("estimated_rate", 0),
            "timestamp": market_data.get("timestamp", 0),
            "time_iso": market_data.get("time_iso", "")
        }
    except Exception as e:
        logger.error(f"Error fetching market data: {e}")

    # æ•´åˆè¾“å‡ºæ•°æ®
    output_data = {
        "account_info": account_info,
        "current_market_data": current_market_data,
        "timeframes": timeframes_data,
        "metadata": {
            "fetch_time": pd.Timestamp.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
            "symbol": symbol,
            "data_version": "1.0"
        }
    }

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    try:
        with open("data.json", "w") as f:
            json.dump(output_data, f, indent=4)
        logger.info("Results saved to data.json")
    except Exception as e:
        logger.error(f"Error saving results: {e}")

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == "--kronos":
        # ç”ŸæˆKronoså¾®è°ƒæ•°æ®
        success = prepare_kronos_training_data()
        sys.exit(0 if success else 1)
    else:
        # é»˜è®¤ç”Ÿæˆå¸¸è§„JSONæ•°æ®
        main()