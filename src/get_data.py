# get_data.py

import os
import json
import pickle
import pandas as pd
from datetime import datetime, timezone
from dotenv import load_dotenv
from logger import setup_logger
from config_loader import ConfigLoader
# from account_fetcher import AccountFetcher  # æš‚æ—¶æ³¨é‡Šï¼Œæµ‹è¯•æ—¶ä¸éœ€è¦è´¦æˆ·ä¿¡æ¯
from data_fetcher import DataFetcher
from technical_indicator import TechnicalIndicator

logger = setup_logger()

# å¯¼å…¥yfinanceç”¨äºè·å–é•¿æœŸå†å²æ•°æ®
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
    logger.info("yfinanceå¯ç”¨ï¼Œæ”¯æŒé•¿æœŸå†å²æ•°æ®è·å–")
except ImportError:
    YFINANCE_AVAILABLE = False
    logger.warning("yfinanceæœªå®‰è£…ï¼Œæ— æ³•è·å–é•¿æœŸå†å²æ•°æ®ã€‚ä½¿ç”¨: pip install yfinance")

def select_indicator_params(config, timeframe):
    """
    æ ¹æ®timeframeé€‰æ‹©å¯¹åº”çš„æŒ‡æ ‡å‚æ•°
    :param config: é…ç½®ä¿¡æ¯
    :param timeframe: æ—¶é—´å‘¨æœŸ
    :return: å¯¹åº”çš„æŒ‡æ ‡å‚æ•°
    """
    for category, intervals in config.get('timeframes', {}).items():
        if timeframe in intervals:
            return config['indicators'].get(category, config['indicators']['midterm'])
    return config['indicators']['midterm']

def fetch_and_process_kline(data_fetcher, symbol, timeframe, config, is_mark_price=False):
    """
    è·å–å’Œå¤„ç†Kçº¿æ•°æ®ï¼ŒåŒ…æ‹¬è·å–å†å²Kçº¿å’Œå½“å‰æœªå®Œç»“Kçº¿
    """
    try:
        # è·å–è¯¥æ—¶é—´å‘¨æœŸçš„é…ç½®
        kline_config = config.get('kline_config', {}).get(timeframe, {})
        fetch_count = kline_config.get('fetch_count', 100)  # é»˜è®¤è·å–100æ¡
        output_count = kline_config.get('output_count', 50) # é»˜è®¤è¾“å‡º50æ¡
        
        logger.info(f"Fetching K-line data for {timeframe}...")
        
        # è·å–å†å²Kçº¿æ•°æ®
        kline_df = data_fetcher.fetch_kline_data(
            instrument_id=symbol,
            bar=timeframe,
            is_mark_price=is_mark_price,
            limit=fetch_count
        )
        
        if kline_df.empty:
            logger.warning(f"No K-line data for {timeframe}.")
            return pd.DataFrame()

        # è·å–å½“å‰æœªå®Œç»“Kçº¿
        current_kline_df = data_fetcher.get_current_kline(symbol, timeframe)
        
        # å¦‚æœæœ‰æœªå®Œç»“Kçº¿ï¼Œæ·»åŠ åˆ°æ•°æ®é›†
        if not current_kline_df.empty:
            kline_df = pd.concat([kline_df, current_kline_df])

        # ç¡®ä¿æŒ‰æ—¶é—´æ’åºå¹¶é‡ç½®ç´¢å¼•
        kline_df = kline_df.sort_values('timestamp', ascending=False)
        kline_df = kline_df.head(fetch_count).sort_values('timestamp')
        kline_df = kline_df.reset_index(drop=True)

        # é€‰æ‹©æŒ‡æ ‡å‚æ•°å¹¶è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        indicator_params = select_indicator_params(config, timeframe)
        
        if 'volume' not in kline_df.columns:
            kline_df['volume'] = 0

        logger.info(f"Calculating indicators for {timeframe}...")
        indicator_calculator = TechnicalIndicator(params=indicator_params)
        kline_df = indicator_calculator.calculate_all(kline_df, category=timeframe)

        # åªä¿ç•™æœ€è¿‘çš„æŒ‡å®šæ•°é‡çš„Kçº¿
        if len(kline_df) > output_count:
            kline_df = kline_df.tail(output_count)

        # è®¾ç½®Kçº¿çŠ¶æ€
        kline_df["is_closed"] = True
        if not current_kline_df.empty:
            kline_df.iloc[-1, kline_df.columns.get_loc("is_closed")] = False

        return kline_df

    except Exception as e:
        logger.error(f"Error processing K-line data for {timeframe}: {e}")
        return pd.DataFrame()

def fetch_long_term_historical_data(symbol="BTC-USD", timeframes=["4h", "1d"], start_date="2021-01-01"):
    """
    ä½¿ç”¨yfinanceè·å–é•¿æœŸå†å²æ•°æ®ï¼ˆä»2021å¹´åˆåˆ°ç°åœ¨ï¼‰
    
    Args:
        symbol: Yahoo Financeçš„äº¤æ˜“å¯¹ç¬¦å·
        timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨ 
        start_date: å¼€å§‹æ—¥æœŸ
        
    Returns:
        Dict[symbol_timeframe, DataFrame]: Kronosæ ¼å¼çš„æ•°æ®å­—å…¸
    """
    if not YFINANCE_AVAILABLE:
        logger.error("yfinanceæœªå®‰è£…ï¼Œæ— æ³•è·å–é•¿æœŸå†å²æ•°æ®")
        return {}
    
    kronos_data = {}
    
    try:
        logger.info(f"ğŸ“Š ä½¿ç”¨Yahoo Financeè·å– {symbol} é•¿æœŸå†å²æ•°æ®...")
        logger.info(f"â° æ—¶é—´èŒƒå›´: {start_date} åˆ°ç°åœ¨")
        
        # è·å–BTCæ•°æ®
        ticker = yf.Ticker(symbol)
        
        for timeframe in timeframes:
            try:
                if timeframe in ["1d", "1D"]:
                    logger.info(f"ğŸ“ˆ è·å–1Då†å²æ•°æ®...")
                    hist = ticker.history(start=start_date, interval="1d")
                    
                    if not hist.empty:
                        # è½¬æ¢ä¸ºKronosæ ¼å¼
                        df_kronos = pd.DataFrame()
                        df_kronos['open'] = hist['Open']
                        df_kronos['high'] = hist['High']
                        df_kronos['low'] = hist['Low']
                        df_kronos['close'] = hist['Close']
                        df_kronos['vol'] = hist['Volume']
                        df_kronos['amt'] = hist['Volume'] * hist['Close']  # æˆäº¤é¢
                        
                        # ç¡®ä¿æ—¶é—´ç´¢å¼•æ­£ç¡®
                        df_kronos.index = pd.to_datetime(df_kronos.index, utc=True)
                        df_kronos.index.name = 'datetime'
                        df_kronos = df_kronos.dropna()
                        
                        symbol_key = f"BTC-USD-SWAP_1D"
                        kronos_data[symbol_key] = df_kronos
                        
                        time_span = df_kronos.index.max() - df_kronos.index.min()
                        logger.info(f"âœ… 1Dæ•°æ®: {len(df_kronos)} æ¡è®°å½•ï¼Œè¦†ç›– {time_span.days} å¤©")
                
                elif timeframe in ["4h", "4H"]:
                    logger.info(f"ğŸ“ˆ ä»1Dæ•°æ®ç”Ÿæˆ4Hæ•°æ®...")
                    
                    # å…ˆè·å–1Dæ•°æ®
                    hist_1d = ticker.history(start=start_date, interval="1d")
                    
                    if not hist_1d.empty:
                        # ä»1Dæ•°æ®ç”Ÿæˆ4Hæ•°æ®
                        df_4h = generate_4h_from_daily_data(hist_1d)
                        
                        if df_4h is not None and not df_4h.empty:
                            symbol_key = f"BTC-USD-SWAP_4H"
                            kronos_data[symbol_key] = df_4h
                            
                            time_span = df_4h.index.max() - df_4h.index.min()
                            logger.info(f"âœ… 4Hæ•°æ®: {len(df_4h)} æ¡è®°å½•ï¼Œè¦†ç›– {time_span.days} å¤©")
                        
            except Exception as e:
                logger.error(f"è·å– {timeframe} æ•°æ®å¤±è´¥: {e}")
                
    except Exception as e:
        logger.error(f"yfinanceæ•°æ®è·å–å¤±è´¥: {e}")
    
    return kronos_data

def generate_4h_from_daily_data(df_1d):
    """ä»1Dæ•°æ®ç”Ÿæˆ4Hæ•°æ®"""
    try:
        # åˆ›å»º4Hæ—¶é—´ç´¢å¼•
        start_time = df_1d.index.min()
        end_time = df_1d.index.max()
        
        # ç”Ÿæˆ4Hæ—¶é—´ç‚¹ (UTCæ—¶é—´ï¼Œæ¯å¤©6ä¸ªç‚¹: 00:00, 04:00, 08:00, 12:00, 16:00, 20:00)
        time_4h = pd.date_range(start=start_time, end=end_time, freq='4h', tz='UTC')
        
        # åˆ›å»º4Hæ•°æ®æ¡†æ¶
        df_4h = pd.DataFrame(index=time_4h)
        
        # ä»1Dæ•°æ®æ’å€¼ç”Ÿæˆ4Hæ•°æ®
        for date in df_1d.index:
            daily_data = df_1d.loc[date]
            
            # ä¸ºè¿™ä¸€å¤©ç”Ÿæˆ6ä¸ª4Hæ•°æ®ç‚¹
            day_start = pd.Timestamp(date.date(), tz='UTC')
            day_4h_times = pd.date_range(start=day_start, periods=6, freq='4h')
            
            for i, time_4h in enumerate(day_4h_times):
                if time_4h in df_4h.index and time_4h <= end_time:
                    # æ¨¡æ‹Ÿæ—¥å†…ä»·æ ¼å˜åŒ–
                    price_var = 0.003 * (i - 2.5)  # Â±0.75%çš„å˜åŒ–
                    
                    df_4h.loc[time_4h, 'open'] = daily_data['Open'] * (1 + price_var * 0.5)
                    df_4h.loc[time_4h, 'high'] = daily_data['High'] * (1 + abs(price_var) * 0.3)
                    df_4h.loc[time_4h, 'low'] = daily_data['Low'] * (1 - abs(price_var) * 0.3)
                    df_4h.loc[time_4h, 'close'] = daily_data['Close'] * (1 + price_var)
                    df_4h.loc[time_4h, 'vol'] = daily_data['Volume'] / 6  # å¹³å‡åˆ†é…æˆäº¤é‡
                    df_4h.loc[time_4h, 'amt'] = df_4h.loc[time_4h, 'vol'] * df_4h.loc[time_4h, 'close']
        
        # æ¸…ç†å’Œæ ¼å¼åŒ–
        df_4h = df_4h.dropna()
        df_4h.index.name = 'datetime'
        
        return df_4h
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆ4Hæ•°æ®å¤±è´¥: {e}")
        return None

def convert_to_kronos_format(df, symbol_name):
    """
    å°†OKX Kçº¿æ•°æ®è½¬æ¢ä¸ºKronoså¾®è°ƒéœ€è¦çš„æ ¼å¼
    
    Args:
        df: OKX Kçº¿æ•°æ®DataFrame
        symbol_name: äº¤æ˜“å¯¹æ ‡è¯†ç¬¦
        
    Returns:
        è½¬æ¢åçš„Kronosæ ¼å¼DataFrame
    """
    if df.empty:
        logger.warning(f"Empty dataframe for {symbol_name}")
        return pd.DataFrame()
    
    try:
        # åˆ›å»ºKronosæ ¼å¼çš„DataFrame
        kronos_df = pd.DataFrame()
        
        # å­—æ®µæ˜ å°„ï¼šOKX -> Kronos
        field_mapping = {
            'open': 'open',
            'high': 'high', 
            'low': 'low',
            'close': 'close',
            'volume': 'vol',              # OKX volume -> Kronos vol
            'currency_volume': 'amt'      # OKX currency_volume -> Kronos amt
        }
        
        # æ˜ å°„å­—æ®µ
        for okx_field, kronos_field in field_mapping.items():
            if okx_field in df.columns:
                kronos_df[kronos_field] = pd.to_numeric(df[okx_field], errors='coerce')
            else:
                logger.warning(f"Field {okx_field} not found in data for {symbol_name}")
                kronos_df[kronos_field] = 0.0
        
        # å¤„ç†æ—¶é—´ç´¢å¼• - è½¬æ¢ä¸ºdatetimeå¹¶è®¾ä¸ºç´¢å¼•
        if 'time_iso' in df.columns:
            kronos_df.index = pd.to_datetime(df['time_iso'])
        elif 'timestamp' in df.columns:
            # å‡è®¾timestampæ˜¯ç§’æˆ–æ¯«ç§’æ—¶é—´æˆ³
            if df['timestamp'].iloc[0] > 1e10:  # æ¯«ç§’æ—¶é—´æˆ³
                kronos_df.index = pd.to_datetime(df['timestamp'], unit='ms')
            else:  # ç§’æ—¶é—´æˆ³
                kronos_df.index = pd.to_datetime(df['timestamp'], unit='s')
        else:
            logger.error(f"No timestamp field found for {symbol_name}")
            return pd.DataFrame()
        
        # ç¡®ä¿ç´¢å¼•åç§°ä¸ºdatetimeï¼ˆKronosè¦æ±‚ï¼‰
        kronos_df.index.name = 'datetime'
        
        # å¡«å……NaNå€¼
        kronos_df = kronos_df.fillna(0.0)
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        for col in ['open', 'high', 'low', 'close', 'vol', 'amt']:
            kronos_df[col] = kronos_df[col].astype(float)
        
        # æŒ‰æ—¶é—´æ’åº
        kronos_df = kronos_df.sort_index()
        
        logger.info(f"Converted {len(kronos_df)} records for {symbol_name}")
        return kronos_df
        
    except Exception as e:
        logger.error(f"Error converting {symbol_name} to Kronos format: {e}")
        return pd.DataFrame()

def fetch_historical_data_for_kronos(data_fetcher, symbol, timeframes, records_per_timeframe=1000):
    """
    è·å–å†å²æ•°æ®ç”¨äºKronoså¾®è°ƒ
    
    Args:
        data_fetcher: DataFetcherå®ä¾‹
        symbol: äº¤æ˜“å¯¹
        timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨
        records_per_timeframe: æ¯ä¸ªæ—¶é—´æ¡†æ¶è·å–çš„è®°å½•æ•°
        
    Returns:
        Dict[symbol_timeframe, DataFrame]: Kronosæ ¼å¼çš„æ•°æ®å­—å…¸
    """
    kronos_data = {}
    
    for timeframe in timeframes:
        try:
            logger.info(f"Fetching {records_per_timeframe} records for {symbol} {timeframe}")
            
            # è·å–å†å²æ•°æ®
            df = data_fetcher.fetch_kline_data(
                instrument_id=symbol,
                bar=timeframe,
                is_mark_price=False,
                limit=records_per_timeframe
            )
            
            if not df.empty:
                # è½¬æ¢ä¸ºKronosæ ¼å¼
                symbol_key = f"{symbol}_{timeframe}"
                kronos_df = convert_to_kronos_format(df, symbol_key)
                
                if not kronos_df.empty:
                    kronos_data[symbol_key] = kronos_df
                    logger.info(f"Successfully processed {len(kronos_df)} records for {symbol_key}")
                else:
                    logger.warning(f"Failed to convert data for {symbol_key}")
            else:
                logger.warning(f"No data fetched for {symbol} {timeframe}")
                
        except Exception as e:
            logger.error(f"Error processing {symbol} {timeframe}: {e}")
    
    return kronos_data

def split_data_for_kronos_training(data_dict, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """
    æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    
    Args:
        data_dict: åŸå§‹æ•°æ®å­—å…¸
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        
    Returns:
        (train_data, val_data, test_data) ä¸‰ä¸ªæ•°æ®å­—å…¸
    """
    if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
        logger.error("Data split ratios must sum to 1.0")
        return {}, {}, {}
    
    train_data = {}
    val_data = {}
    test_data = {}
    
    for symbol, df in data_dict.items():
        if df.empty:
            continue
            
        n_total = len(df)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)
        
        # æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²ï¼ˆæ—©æœŸ->è®­ç»ƒï¼Œä¸­æœŸ->éªŒè¯ï¼Œæœ€æ–°->æµ‹è¯•ï¼‰
        train_data[symbol] = df.iloc[:n_train].copy()
        val_data[symbol] = df.iloc[n_train:n_train+n_val].copy()
        test_data[symbol] = df.iloc[n_train+n_val:].copy()
        
        logger.info(f"Split {symbol}: train={len(train_data[symbol])}, "
                   f"val={len(val_data[symbol])}, test={len(test_data[symbol])}")
    
    return train_data, val_data, test_data

def save_kronos_datasets(train_data, val_data, test_data, output_dir="data/kronos_datasets"):
    """
    ä¿å­˜Kronosæ ¼å¼çš„æ•°æ®é›†ä¸ºpickleæ–‡ä»¶
    
    Args:
        train_data: è®­ç»ƒæ•°æ®å­—å…¸
        val_data: éªŒè¯æ•°æ®å­—å…¸
        test_data: æµ‹è¯•æ•°æ®å­—å…¸
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ä¸‰ä¸ªæ•°æ®é›†
        datasets = {
            'train_data.pkl': train_data,
            'val_data.pkl': val_data,
            'test_data.pkl': test_data
        }
        
        for filename, data in datasets.items():
            filepath = os.path.join(output_dir, filename)
            with open(filepath, 'wb') as f:
                pickle.dump(data, f)
            logger.info(f"Saved {filename} with {len(data)} symbols")
        
        # ä¿å­˜æ•°æ®æ‘˜è¦
        summary = {
            'train_symbols': len(train_data),
            'val_symbols': len(val_data), 
            'test_symbols': len(test_data),
            'total_train_records': sum(len(df) for df in train_data.values()),
            'total_val_records': sum(len(df) for df in val_data.values()),
            'total_test_records': sum(len(df) for df in test_data.values()),
            'created_at': datetime.now().isoformat(),
            'symbols': list(train_data.keys())
        }
        
        summary_path = os.path.join(output_dir, 'dataset_summary.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Successfully saved Kronos datasets to {output_dir}")
        logger.info(f"Summary: {summary}")
        return True
        
    except Exception as e:
        logger.error(f"Error saving Kronos datasets: {e}")
        return False

def fetch_kronos_prediction_data_batch(data_fetcher, symbol, timeframe, config):
    """
    åˆ†æ‰¹è·å–OKX Kçº¿æ•°æ®ï¼Œå°½å¯èƒ½è·å–ç›®æ ‡æ•°é‡çš„æ•°æ®
    
    Args:
        data_fetcher: DataFetcherå®ä¾‹
        symbol: äº¤æ˜“å¯¹
        timeframe: æ—¶é—´æ¡†æ¶
        config: æ—¶é—´æ¡†æ¶é…ç½®
        
    Returns:
        DataFrame: åˆå¹¶åçš„Kçº¿æ•°æ®
    """
    all_data = []
    
    # ä»é…ç½®è·å–å‚æ•°
    target_count = config.get('target_count', 512)
    batch_size = config.get('batch_size', 300)
    max_batches = config.get('max_batches', 3)
    
    total_fetched = 0
    
    logger.info(f"ğŸ”„ Starting optimized batch fetch for {symbol} {timeframe}")
    logger.info(f"   Target: {target_count} records, Batch size: {batch_size}, Max batches: {max_batches}")
    
    # ç¬¬ä¸€æ‰¹ï¼šè·å–æœ€æ–°æ•°æ®
    try:
        df_latest = data_fetcher.fetch_kline_data(
            instrument_id=symbol,
            bar=timeframe,
            is_mark_price=False,
            limit=batch_size
        )
        
        if not df_latest.empty:
            all_data.append(df_latest)
            total_fetched = len(df_latest)
            logger.info(f"   Batch 1: {len(df_latest)} records (latest)")
            
            # å¦‚æœç¬¬ä¸€æ‰¹å°±è¶³å¤Ÿäº†
            if total_fetched >= target_count:
                combined_df = df_latest.tail(target_count)
                logger.info(f"âœ… Target reached with first batch: {len(combined_df)} records")
                return combined_df
            
            # è·å–æœ€æ—©çš„æ—¶é—´æˆ³ï¼Œç”¨äºè·å–æ›´æ—©çš„æ•°æ®
            earliest_time = df_latest['timestamp'].min()
            
            # ç»§ç»­è·å–æ›´æ—©çš„æ•°æ®
            batch_count = 1
            current_before = earliest_time
            
            while total_fetched < target_count and batch_count < max_batches:
                batch_count += 1
                
                # å°è¯•è·å–æ›´æ—©çš„æ•°æ®ï¼Œä½¿ç”¨beforeå‚æ•°
                try:
                    # ä½¿ç”¨beforeå‚æ•°è·å–æ›´æ—©çš„æ•°æ®
                    df_earlier = data_fetcher.fetch_kline_data(
                        instrument_id=symbol,
                        bar=timeframe,
                        is_mark_price=False,
                        limit=batch_size,
                        before=current_before
                    )
                    
                    if not df_earlier.empty:
                        # è¿‡æ»¤é‡å¤æ•°æ®
                        df_earlier = df_earlier[df_earlier['timestamp'] < current_before]
                        
                        if not df_earlier.empty:
                            all_data.insert(0, df_earlier)  # æ’å…¥åˆ°å‰é¢
                            total_fetched += len(df_earlier)
                            # æ›´æ–°beforeå‚æ•°ä¸ºè¿™æ‰¹æ•°æ®çš„æœ€æ—©æ—¶é—´
                            current_before = df_earlier['timestamp'].min()
                            earliest_time = min(earliest_time, current_before)
                            logger.info(f"   Batch {batch_count}: {len(df_earlier)} records (earlier)")
                        else:
                            logger.info(f"   Batch {batch_count}: No new data found")
                            break
                    else:
                        logger.info(f"   Batch {batch_count}: No data returned")
                        break
                        
                except Exception as e:
                    logger.warning(f"   Batch {batch_count} failed: {e}")
                    break
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            # æŒ‰æ—¶é—´æ’åºå¹¶å»é‡
            combined_df = combined_df.sort_values('timestamp').drop_duplicates(subset=['timestamp'])
            
            # å–æœ€æ–°çš„target_countæ¡æ•°æ®
            if len(combined_df) > target_count:
                combined_df = combined_df.tail(target_count)
            
            logger.info(f"âœ… Batch fetch completed: {len(combined_df)} records total")
            return combined_df
        else:
            logger.warning("No data fetched in any batch")
            return pd.DataFrame()
            
    except Exception as e:
        logger.error(f"Error in batch fetch: {e}")
        return pd.DataFrame()

def fetch_kronos_prediction_data(config, timeframes=None):
    """
    ä¸“é—¨ä¸ºKronoså®æ—¶é¢„æµ‹è·å–æœ€æ–°512æ¡Kçº¿æ•°æ®
    ä½¿ç”¨ä¼˜åŒ–çš„åˆ†æ‰¹è·å–ç­–ç•¥
    
    Args:
        config: é…ç½®å¯¹è±¡
        timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä»é…ç½®ä¸­è·å–
        
    Returns:
        Dict[timeframe, DataFrame]: æ¯ä¸ªæ—¶é—´æ¡†æ¶çš„Kronosæ ¼å¼æ•°æ®
    """
    logger.info("ğŸ”® Starting Kronos prediction data fetch (optimized for 512 records)...")
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    api_key = os.getenv("OKX_API_KEY")
    secret_key = os.getenv("OKX_API_SECRET")
    passphrase = os.getenv("OKX_API_PASSPHRASE")
    
    if not all([api_key, secret_key, passphrase]):
        logger.error("API credentials missing.")
        return {}

    # åˆå§‹åŒ–æ•°æ®è·å–å™¨
    try:
        data_fetcher = DataFetcher(api_key, secret_key, passphrase)
        logger.info("DataFetcher initialized successfully.")
    except Exception as e:
        logger.error(f"Error initializing DataFetcher: {e}")
        return {}

    # è·å–æ—¶é—´æ¡†æ¶é…ç½®
    if timeframes is None:
        timeframes = config.get('timeframes', {}).get('kronos_prediction', ['4H', '1D'])
    
    kronos_prediction_config = config.get('kronos_prediction_config', {})
    
    prediction_data = {}
    
    for timeframe in timeframes:
        try:
            # è·å–è¯¥æ—¶é—´æ¡†æ¶çš„é…ç½®
            tf_config = kronos_prediction_config.get(timeframe, {})
            symbol = tf_config.get('symbol', 'BTC-USD-SWAP')
            fetch_count = tf_config.get('fetch_count', 512)
            
            logger.info(f"ğŸ“Š Fetching {fetch_count} records for {symbol} {timeframe}...")
            
            # ä½¿ç”¨ç»„åˆè·å–ç­–ç•¥ï¼ˆlive + historyï¼‰
            df = data_fetcher.fetch_combined_kline_data(symbol, timeframe, fetch_count)
            
            if not df.empty:
                # è½¬æ¢ä¸ºKronosæ ¼å¼
                symbol_key = f"{symbol}_{timeframe}"
                kronos_df = convert_to_kronos_format(df, symbol_key)
                
                if not kronos_df.empty:
                    prediction_data[timeframe] = kronos_df
                    logger.info(f"âœ… Successfully processed {len(kronos_df)} records for {timeframe}")
                    
                    # æ˜¾ç¤ºæ•°æ®æ—¶é—´èŒƒå›´
                    start_time = kronos_df.index[0]
                    end_time = kronos_df.index[-1]
                    logger.info(f"   ğŸ“… Time range: {start_time} to {end_time}")
                    
                    # æ˜¾ç¤ºæ•°æ®å¯†åº¦ä¿¡æ¯
                    time_span = end_time - start_time
                    if timeframe == '4H':
                        expected_records = time_span.total_seconds() / (4 * 3600)
                        logger.info(f"   ğŸ“ˆ Data density: {len(kronos_df)}/{expected_records:.0f} expected 4H records")
                    elif timeframe == '1D':
                        expected_records = time_span.days
                        logger.info(f"   ğŸ“ˆ Data density: {len(kronos_df)}/{expected_records:.0f} expected daily records")
                else:
                    logger.warning(f"Failed to convert data for {timeframe}")
            else:
                logger.warning(f"No data fetched for {symbol} {timeframe}")
                
        except Exception as e:
            logger.error(f"Error processing {timeframe}: {e}")
    
    return prediction_data

def save_kronos_prediction_data(prediction_data, output_dir="./Kronos/finetune/data"):
    """
    ä¿å­˜Kronosé¢„æµ‹æ•°æ®åˆ°æŒ‡å®šç›®å½•
    
    Args:
        prediction_data: é¢„æµ‹æ•°æ®å­—å…¸
        output_dir: è¾“å‡ºç›®å½•
    """
    try:
        os.makedirs(output_dir, exist_ok=True)
        
        for timeframe, df in prediction_data.items():
            # ä¿å­˜ä¸ºpickleæ ¼å¼ï¼ˆKronosè®­ç»ƒæ ¼å¼ï¼‰
            filename = f"prediction_data_{timeframe.lower()}.pkl"
            filepath = os.path.join(output_dir, filename)
            
            # è½¬æ¢ä¸ºKronosè®­ç»ƒéœ€è¦çš„æ ¼å¼
            data_dict = {
                f"BTC-USD-SWAP_{timeframe}": df
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(data_dict, f)
            
            logger.info(f"ğŸ’¾ Saved {timeframe} prediction data to {filepath}")
            
            # åŒæ—¶ä¿å­˜CSVæ ¼å¼ä¾¿äºæŸ¥çœ‹
            csv_filepath = os.path.join(output_dir, f"prediction_data_{timeframe.lower()}.csv")
            df.to_csv(csv_filepath)
            logger.info(f"ğŸ“„ Saved {timeframe} CSV data to {csv_filepath}")
            
    except Exception as e:
        logger.error(f"Error saving prediction data: {e}")

def prepare_kronos_training_data():
    """
    ä¸“é—¨ä¸ºKronoså¾®è°ƒå‡†å¤‡æ•°æ®çš„å‡½æ•°
    æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è‡ªå®šä¹‰å‚æ•°ï¼Œå®ç°å¤šæ¨¡å‹é…ç½®é©±åŠ¨
    
    æ”¯æŒçš„ç¯å¢ƒå˜é‡:
    - KRONOS_OUTPUT_DIR: è‡ªå®šä¹‰è¾“å‡ºç›®å½•
    - KRONOS_TIMEFRAMES: æ—¶é—´æ¡†æ¶åˆ—è¡¨ (é€—å·åˆ†éš”ï¼Œå¦‚ "4H,1D")
    - KRONOS_RECORDS_PER_TF: æ¯ä¸ªæ—¶é—´æ¡†æ¶çš„è®°å½•æ•°
    """
    logger.info("ğŸ¯ Starting Kronos training data preparation...")
    
    # åŠ è½½é…ç½®
    try:
        config_loader = ConfigLoader("../config/config.yaml")
        config = config_loader.load_config()
        logger.info("Configuration loaded.")
    except Exception as e:
        logger.error(f"Error loading configuration: {e}")
        return False

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    api_key = os.getenv("OKX_API_KEY")
    secret_key = os.getenv("OKX_API_SECRET")
    passphrase = os.getenv("OKX_API_PASSPHRASE")
    if not all([api_key, secret_key, passphrase]):
        logger.error("API credentials missing.")
        return False

    # ä»ç¯å¢ƒå˜é‡è·å–è‡ªå®šä¹‰å‚æ•°
    custom_output_dir = os.getenv("KRONOS_OUTPUT_DIR")
    custom_timeframes = os.getenv("KRONOS_TIMEFRAMES")
    custom_records_per_tf = os.getenv("KRONOS_RECORDS_PER_TF")
    use_long_term_data = os.getenv("KRONOS_USE_LONG_TERM", "false").lower() == "true"
    
    # å®šä¹‰Kronoså¾®è°ƒçš„æ—¶é—´æ¡†æ¶å’Œå‚æ•°ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
    symbol = "BTC-USD-SWAP"
    
    # å¤„ç†æ—¶é—´æ¡†æ¶å‚æ•°
    if custom_timeframes:
        timeframes = [tf.strip() for tf in custom_timeframes.split(",")]
        logger.info(f"ğŸ”§ Using custom timeframes from environment: {timeframes}")
    else:
        timeframes = ["1H", "4H", "1D"]  # é»˜è®¤æ—¶é—´æ¡†æ¶
        
    # å¤„ç†è®°å½•æ•°å‚æ•°
    if custom_records_per_tf:
        try:
            records_per_timeframe = int(custom_records_per_tf)
            logger.info(f"ğŸ”§ Using custom records per timeframe from environment: {records_per_timeframe}")
        except ValueError:
            logger.warning(f"Invalid KRONOS_RECORDS_PER_TF value: {custom_records_per_tf}, using default")
            records_per_timeframe = 2000
    else:
        records_per_timeframe = 2000  # é»˜è®¤å€¼
    
    # å¤„ç†è¾“å‡ºç›®å½•å‚æ•°
    if custom_output_dir:
        output_dir = custom_output_dir
        logger.info(f"ğŸ”§ Using custom output directory from environment: {output_dir}")
    else:
        output_dir = "data/kronos_datasets"  # é»˜è®¤è¾“å‡ºç›®å½•
        
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é•¿æœŸå†å²æ•°æ®
    if use_long_term_data and YFINANCE_AVAILABLE:
        logger.info("ğŸš€ ä½¿ç”¨yfinanceè·å–é•¿æœŸå†å²æ•°æ® (2021å¹´åˆè‡³ä»Š)...")
        # ç›´æ¥ä½¿ç”¨yfinanceè·å–é•¿æœŸæ•°æ®ï¼Œè·³è¿‡DataFetcher
        kronos_data = fetch_long_term_historical_data("BTC-USD", ["4h", "1d"], "2021-01-01")
    else:
        logger.info("ä½¿ç”¨OKX APIè·å–è¿‘æœŸæ•°æ®...")
        # åˆå§‹åŒ–æ•°æ®è·å–å™¨
        try:
            data_fetcher = DataFetcher(api_key, secret_key, passphrase)
            logger.info("DataFetcher initialized successfully")
        except Exception as e:
            logger.error(f"Error initializing DataFetcher: {e}")
            return False
        
        # è·å–å†å²æ•°æ®
        kronos_data = fetch_historical_data_for_kronos(
            data_fetcher, symbol, timeframes, records_per_timeframe
        )
    
    logger.info(f"ğŸ“Š Fetching data for {symbol}")
    logger.info(f"â±ï¸ Timeframes: {timeframes}")
    logger.info(f"ğŸ“ˆ Records per timeframe: {records_per_timeframe}")
    logger.info(f"ğŸ’¾ Output directory: {output_dir}")
    
    # è·å–å†å²æ•°æ®
    kronos_data = fetch_historical_data_for_kronos(
        data_fetcher, symbol, timeframes, records_per_timeframe
    )
    
    if not kronos_data:
        logger.error("No data fetched for Kronos training")
        return False
    
    logger.info(f"Fetched data for {len(kronos_data)} timeframe combinations")
    
    # åˆ†å‰²æ•°æ®é›†
    logger.info("Splitting datasets...")
    train_data, val_data, test_data = split_data_for_kronos_training(kronos_data)
    
    # ä¿å­˜æ•°æ®é›†åˆ°æŒ‡å®šç›®å½•
    logger.info(f"ğŸ’¾ Saving Kronos datasets to {output_dir}...")
    success = save_kronos_datasets(train_data, val_data, test_data, output_dir)
    
    if success:
        logger.info("ğŸ‰ Kronos training data preparation completed!")
        logger.info(f"ğŸ“ Data saved to: {output_dir}")
        logger.info("ğŸ“„ Files created:")
        logger.info("   - train_data.pkl")
        logger.info("   - val_data.pkl") 
        logger.info("   - test_data.pkl")
        logger.info("   - dataset_summary.json")
        
        # æ˜¾ç¤ºç¯å¢ƒå˜é‡é…ç½®æ‘˜è¦
        if any([custom_output_dir, custom_timeframes, custom_records_per_tf]):
            logger.info("ğŸ”§ Environment customizations applied:")
            if custom_output_dir:
                logger.info(f"   - Custom output directory: {custom_output_dir}")
            if custom_timeframes:
                logger.info(f"   - Custom timeframes: {custom_timeframes}")
            if custom_records_per_tf:
                logger.info(f"   - Custom records per timeframe: {custom_records_per_tf}")
        
        logger.info("ğŸš€ Ready for Kronos fine-tuning!")
        return True
    else:
        logger.error("âŒ Failed to save Kronos datasets")
        return False

def main():
    logger.info("Starting the data fetching process...")
    
    # åŠ è½½é…ç½®
    try:
        config_loader = ConfigLoader("../config/config.yaml")
        config = config_loader.load_config()
        logger.info("Configuration loaded.")
    except Exception as e:
        logger.error(f"Error loading configuration: {e}")
        return

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    api_key = os.getenv("OKX_API_KEY")
    secret_key = os.getenv("OKX_API_SECRET")
    passphrase = os.getenv("OKX_API_PASSPHRASE")
    if not all([api_key, secret_key, passphrase]):
        logger.error("API credentials missing.")
        return

    # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
    try:
        account_fetcher = AccountFetcher(api_key, secret_key, passphrase)
        data_fetcher = DataFetcher(api_key, secret_key, passphrase)
    except Exception as e:
        logger.error(f"Error initializing modules: {e}")
        return

    # è·å–è´¦æˆ·è¯¦ç»†ä¿¡æ¯
    logger.info("Fetching account data...")
    account_info = {"balance": 0, "positions": []}
    try:
        acct_bal = account_fetcher.get_balance()
        account_info = {
            "balance": acct_bal.get("balance", 0),
            "available_balance": acct_bal.get("available_balance", 0),
            "margin_ratio": acct_bal.get("margin_ratio", 0),
            "margin_frozen": acct_bal.get("margin_frozen", 0),
            "total_equity": acct_bal.get("total_equity", 0),
            "unrealized_pnl": acct_bal.get("unrealized_pnl", 0),
            "positions": account_fetcher.get_detailed_positions()
        }
    except Exception as e:
        logger.error(f"Error fetching account data: {e}")

    # è·å–æ‰€æœ‰éœ€è¦å¤„ç†çš„æ—¶é—´å‘¨æœŸ
    all_timeframes = []
    for category, tfs in config.get('timeframes', {}).items():
        all_timeframes.extend(tfs)

    symbol = "BTC-USD-SWAP"  # å¯ä»configæˆ–envä¸­åŠ è½½

    # å¤„ç†Kçº¿æ•°æ®
    timeframes_data = {}
    logger.info("Fetching actual price timeframes data...")
    for tf in all_timeframes:
        df = fetch_and_process_kline(data_fetcher, symbol, tf, config, is_mark_price=False)
        if not df.empty:
            timeframes_data[tf] = {
                "type": "actual_price",
                "indicators_params": select_indicator_params(config, tf),
                "data": df.to_dict(orient="records")
            }

    # è·å–å½“å‰å¸‚åœºæ•°æ®
    current_market_data = {}
    try:
        # è·å–å¸‚åœºåŸºç¡€æ•°æ®
        market_data = data_fetcher.fetch_ticker(symbol)
        # è·å–èµ„é‡‘è´¹ç‡æ•°æ®
        funding_data = data_fetcher.fetch_funding_rate(symbol)
        
        current_market_data = {
            "last_price": market_data.get("last_price", 0),
            "best_bid": market_data.get("best_bid", 0),
            "best_ask": market_data.get("best_ask", 0),
            "24h_high": market_data.get("24h_high", 0),
            "24h_low": market_data.get("24h_low", 0),
            "24h_volume": market_data.get("24h_volume", 0),
            "24h_turnover": market_data.get("24h_turnover", 0),
            "open_interest": market_data.get("open_interest", 0),
            "funding_rate": funding_data.get("funding_rate", 0),
            "next_funding_time": funding_data.get("next_funding_time", ""),
            "estimated_rate": funding_data.get("estimated_rate", 0),
            "timestamp": market_data.get("timestamp", 0),
            "time_iso": market_data.get("time_iso", "")
        }
    except Exception as e:
        logger.error(f"Error fetching market data: {e}")

    # æ•´åˆè¾“å‡ºæ•°æ®
    output_data = {
        "account_info": account_info,
        "current_market_data": current_market_data,
        "timeframes": timeframes_data,
        "metadata": {
            "fetch_time": pd.Timestamp.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
            "symbol": symbol,
            "data_version": "1.0"
        }
    }

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    try:
        with open("data.json", "w") as f:
            json.dump(output_data, f, indent=4)
        logger.info("Results saved to data.json")
    except Exception as e:
        logger.error(f"Error saving results: {e}")

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == "--kronos":
        # ç”ŸæˆKronoså¾®è°ƒæ•°æ®
        success = prepare_kronos_training_data()
        sys.exit(0 if success else 1)
    else:
        # é»˜è®¤ç”Ÿæˆå¸¸è§„JSONæ•°æ®
        main()