#!/usr/bin/env python3
"""
è°ƒè¯•è„šæœ¬ï¼šæ£€æŸ¥tokenizerè¾“å‡ºæ ¼å¼
"""

import sys
import torch
sys.path.append("./Kronos/")
sys.path.append("./Kronos/finetune/")

from config_model_A_4H import Config
from dataset import QlibDataset
from model.kronos import KronosTokenizer
from torch.utils.data import DataLoader

def debug_tokenizer():
    """è°ƒè¯•tokenizerè¾“å‡º"""
    print("ğŸ” è°ƒè¯•Tokenizerè¾“å‡ºæ ¼å¼")
    print("=" * 50)
    
    # ç›´æ¥åˆ›å»ºé…ç½®
    config = Config()
    # ä¿®æ”¹æ•°æ®é›†è·¯å¾„
    config.dataset_path = "./data/kronos_datasets/model_a_4h_full"
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")
    
    # åŠ è½½tokenizer
    tokenizer_path = "./Kronos/finetune/outputs/models/model_a_4h/tokenizer_model_a_4h/checkpoints/best_model"
    print(f"åŠ è½½tokenizer: {tokenizer_path}")
    tokenizer = KronosTokenizer.from_pretrained(tokenizer_path)
    tokenizer.to(device)
    tokenizer.eval()
    
    print(f"Tokenizerå‚æ•°:")
    print(f"  s1_bits: {tokenizer.s1_bits}")
    print(f"  s2_bits: {tokenizer.s2_bits}")
    print(f"  codebook_dim: {tokenizer.codebook_dim}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    dataset = QlibDataset('train', config=config)
    dataloader = DataLoader(dataset, batch_size=2, shuffle=False)
    
    # è·å–ä¸€ä¸ªbatch
    batch = next(iter(dataloader))
    x_tensor, x_stamp_tensor = batch
    x_tensor = x_tensor.to(device)
    
    print(f"\nè¾“å…¥æ•°æ®å½¢çŠ¶:")
    print(f"  x_tensor: {x_tensor.shape}")
    print(f"  x_stamp_tensor: {x_stamp_tensor.shape}")
    
    # è¿è¡Œtokenizer
    with torch.no_grad():
        outputs, bsq_loss, quantized, z_indices = tokenizer(x_tensor)
        
        print(f"\nTokenizerè¾“å‡º:")
        print(f"  outputsç±»å‹: {type(outputs)}")
        if isinstance(outputs, tuple):
            print(f"  outputsé•¿åº¦: {len(outputs)}")
            for i, out in enumerate(outputs):
                print(f"    outputs[{i}]: {out.shape}")
        else:
            print(f"  outputså½¢çŠ¶: {outputs.shape}")
        
        print(f"  bsq_loss: {bsq_loss}")
        print(f"  quantizedå½¢çŠ¶: {quantized.shape}")
        print(f"  z_indiceså½¢çŠ¶: {z_indices.shape}")
        
        print(f"\nz_indicesè¯¦ç»†ä¿¡æ¯:")
        print(f"  æ•°æ®ç±»å‹: {z_indices.dtype}")
        print(f"  æ•°å€¼èŒƒå›´: [{z_indices.min().item():.4f}, {z_indices.max().item():.4f}]")
        print(f"  å‰å‡ ä¸ªå€¼: {z_indices[0, 0, :10]}")
        
        # å°è¯•åˆ†ç¦»s1å’Œs2
        s1_bits = tokenizer.s1_bits
        s2_bits = tokenizer.s2_bits
        
        print(f"\nåˆ†ç¦»s1å’Œs2:")
        print(f"  s1_bits: {s1_bits}, s2_bits: {s2_bits}")
        
        if z_indices.dim() == 3:
            s1_indices = z_indices[:, :, :s1_bits]
            s2_indices = z_indices[:, :, s1_bits:]
            
            print(f"  s1_indiceså½¢çŠ¶: {s1_indices.shape}")
            print(f"  s2_indiceså½¢çŠ¶: {s2_indices.shape}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯äºŒè¿›åˆ¶æ•°æ®
            unique_values = torch.unique(z_indices)
            print(f"  z_indiceså”¯ä¸€å€¼: {unique_values}")
            
            if len(unique_values) == 2 and 0 in unique_values and 1 in unique_values:
                print("  âœ… z_indicesæ˜¯äºŒè¿›åˆ¶æ•°æ®")
                # å°†äºŒè¿›åˆ¶è½¬æ¢ä¸ºæ•´æ•°
                s1_ids = torch.sum(s1_indices * (2 ** torch.arange(s1_bits, device=device)), dim=-1)
                s2_ids = torch.sum(s2_indices * (2 ** torch.arange(s2_bits, device=device)), dim=-1)
                
                print(f"  s1_idså½¢çŠ¶: {s1_ids.shape}, èŒƒå›´: [{s1_ids.min()}, {s1_ids.max()}]")
                print(f"  s2_idså½¢çŠ¶: {s2_ids.shape}, èŒƒå›´: [{s2_ids.min()}, {s2_ids.max()}]")
            else:
                print("  âŒ z_indicesä¸æ˜¯äºŒè¿›åˆ¶æ•°æ®")
                print("  å°è¯•ä½¿ç”¨argmax...")
                s1_ids = torch.argmax(s1_indices, dim=-1)
                s2_ids = torch.argmax(s2_indices, dim=-1)
                print(f"  s1_idså½¢çŠ¶: {s1_ids.shape}")
                print(f"  s2_idså½¢çŠ¶: {s2_ids.shape}")
        else:
            print(f"  âŒ z_indicesç»´åº¦ä¸æ˜¯3: {z_indices.dim()}")

if __name__ == "__main__":
    debug_tokenizer()