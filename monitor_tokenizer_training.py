#!/usr/bin/env python3
"""
Kronos Tokenizerè®­ç»ƒçŠ¶æ€ç›‘æ§è„šæœ¬
ç”¨äºç›‘æ§æ¨¡å‹A (4H) Tokenizerçš„è®­ç»ƒè¿›åº¦
"""

import os
import time
import json
from datetime import datetime

def monitor_training_progress():
    """ç›‘æ§è®­ç»ƒè¿›åº¦"""
    
    # è®­ç»ƒè¾“å‡ºç›®å½•
    model_dir = "./Kronos/finetune/outputs/models/model_a_4h/tokenizer_model_a_4h"
    checkpoints_dir = os.path.join(model_dir, "checkpoints")
    
    print("ğŸ” Kronos Tokenizer (Model A - 4H) è®­ç»ƒç›‘æ§")
    print("=" * 60)
    
    while True:
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰checkpointæ–‡ä»¶
            if os.path.exists(checkpoints_dir):
                checkpoints = [f for f in os.listdir(checkpoints_dir) if f.startswith('epoch_')]
                if checkpoints:
                    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[1]))
                    print(f"ğŸ“Š æœ€æ–°checkpoint: {latest_checkpoint}")
                    
                    # æ£€æŸ¥best_model
                    best_model_path = os.path.join(checkpoints_dir, "best_model")
                    if os.path.exists(best_model_path):
                        print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path}")
                        
                        # å°è¯•è¯»å–è®­ç»ƒæ—¥å¿—
                        log_file = os.path.join(model_dir, "training_log.txt")
                        if os.path.exists(log_file):
                            with open(log_file, 'r', encoding='utf-8') as f:
                                lines = f.readlines()
                                if lines:
                                    print(f"ğŸ“ æœ€æ–°æ—¥å¿—: {lines[-1].strip()}")
                else:
                    print("â³ ç­‰å¾…è®­ç»ƒå¼€å§‹...")
            else:
                print("â³ è®­ç»ƒå°šæœªå¼€å§‹ï¼Œç­‰å¾…è¾“å‡ºç›®å½•åˆ›å»º...")
            
            # æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    gpu_used = torch.cuda.memory_allocated(0) / 1024**3
                    print(f"ğŸ–¥ï¸  GPUå†…å­˜ä½¿ç”¨: {gpu_used:.1f}GB / {gpu_memory:.1f}GB")
            except:
                pass
            
            print(f"ğŸ•’ æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("-" * 60)
            
            # ç­‰å¾…30ç§’åå†æ¬¡æ£€æŸ¥
            time.sleep(30)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç›‘æ§å·²åœæ­¢")
            break
        except Exception as e:
            print(f"âŒ ç›‘æ§é”™è¯¯: {e}")
            time.sleep(10)

def check_training_status():
    """å¿«é€Ÿæ£€æŸ¥è®­ç»ƒçŠ¶æ€"""
    model_dir = "./Kronos/finetune/outputs/models/model_a_4h/tokenizer_model_a_4h"
    checkpoints_dir = os.path.join(model_dir, "checkpoints")
    
    print("ğŸ” å¿«é€ŸçŠ¶æ€æ£€æŸ¥")
    print("=" * 40)
    
    if os.path.exists(model_dir):
        print(f"âœ… è®­ç»ƒç›®å½•å­˜åœ¨: {model_dir}")
        
        if os.path.exists(checkpoints_dir):
            checkpoints = [f for f in os.listdir(checkpoints_dir) if f.startswith('epoch_')]
            print(f"ğŸ“Š Checkpointæ•°é‡: {len(checkpoints)}")
            
            if checkpoints:
                latest = max(checkpoints, key=lambda x: int(x.split('_')[1]))
                epoch_num = latest.split('_')[1]
                print(f"ğŸ“ˆ æœ€æ–°epoch: {epoch_num}")
            
            best_model_path = os.path.join(checkpoints_dir, "best_model")
            if os.path.exists(best_model_path):
                print("âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜")
            else:
                print("â³ æœ€ä½³æ¨¡å‹å°šæœªä¿å­˜")
        else:
            print("â³ Checkpointsç›®å½•ä¸å­˜åœ¨")
    else:
        print("âŒ è®­ç»ƒç›®å½•ä¸å­˜åœ¨")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--quick":
        check_training_status()
    else:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python monitor_tokenizer_training.py          # æŒç»­ç›‘æ§")
        print("  python monitor_tokenizer_training.py --quick  # å¿«é€Ÿæ£€æŸ¥")
        print()
        
        choice = input("é€‰æ‹©æ¨¡å¼ (1=æŒç»­ç›‘æ§, 2=å¿«é€Ÿæ£€æŸ¥): ").strip()
        if choice == "1":
            monitor_training_progress()
        elif choice == "2":
            check_training_status()
        else:
            print("æ— æ•ˆé€‰æ‹©")