#!/usr/bin/env python3
"""
Kçº¿æ•°æ®è¿ç»­æ€§åˆ†æè„šæœ¬
æ£€æŸ¥512æ¡Kçº¿æ•°æ®æ˜¯å¦è¿ç»­ã€æœ‰æ— ä¸­æ–­æˆ–é‡å 
"""

import os
import sys
import pandas as pd
import pickle
from datetime import datetime, timedelta

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append("./src")
from logger import setup_logger

logger = setup_logger()

def analyze_kline_continuity(data_file, timeframe):
    """
    åˆ†æKçº¿æ•°æ®çš„è¿ç»­æ€§
    
    Args:
        data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
        timeframe: æ—¶é—´æ¡†æ¶ (4H, 1D)
    """
    logger.info(f"ğŸ” åˆ†æ {timeframe} Kçº¿æ•°æ®è¿ç»­æ€§")
    logger.info("=" * 60)
    
    try:
        # è¯»å–pickleæ•°æ®
        with open(data_file, 'rb') as f:
            data_dict = pickle.load(f)
        
        # è·å–æ•°æ®
        symbol_key = f"BTC-USD-SWAP_{timeframe}"
        if symbol_key not in data_dict:
            logger.error(f"âŒ æœªæ‰¾åˆ° {symbol_key} æ•°æ®")
            return False
        
        df = data_dict[symbol_key]
        logger.info(f"ğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯:")
        logger.info(f"   æ€»è®°å½•æ•°: {len(df)}")
        logger.info(f"   æ—¶é—´èŒƒå›´: {df.index[0]} åˆ° {df.index[-1]}")
        
        # è½¬æ¢ä¸ºæ—¶é—´æˆ³è¿›è¡Œåˆ†æ
        timestamps = pd.to_datetime(df.index)
        timestamps_sorted = timestamps.sort_values()
        
        # è®¡ç®—æ—¶é—´é—´éš”
        time_diffs = timestamps_sorted.diff().dropna()
        
        # æ ¹æ®æ—¶é—´æ¡†æ¶ç¡®å®šé¢„æœŸé—´éš”
        if timeframe == '4H':
            expected_interval = timedelta(hours=4)
            interval_name = "4å°æ—¶"
        elif timeframe == '1D':
            expected_interval = timedelta(days=1)
            interval_name = "1å¤©"
        else:
            logger.error(f"âŒ ä¸æ”¯æŒçš„æ—¶é—´æ¡†æ¶: {timeframe}")
            return False
        
        logger.info(f"\nğŸ“ˆ æ—¶é—´é—´éš”åˆ†æ (é¢„æœŸé—´éš”: {interval_name}):")
        
        # ç»Ÿè®¡ä¸åŒé—´éš”çš„æ•°é‡
        interval_counts = {}
        gaps = []
        overlaps = []
        
        for i, diff in enumerate(time_diffs):
            diff_hours = diff.total_seconds() / 3600
            
            if timeframe == '4H':
                if diff_hours == 4:
                    interval_counts['æ­£å¸¸'] = interval_counts.get('æ­£å¸¸', 0) + 1
                elif diff_hours > 4:
                    interval_counts['é—´éš”è¿‡å¤§'] = interval_counts.get('é—´éš”è¿‡å¤§', 0) + 1
                    gaps.append({
                        'position': i,
                        'time': timestamps_sorted.iloc[i],
                        'gap_hours': diff_hours,
                        'missing_periods': int(diff_hours / 4) - 1
                    })
                elif diff_hours < 4:
                    interval_counts['é—´éš”è¿‡å°'] = interval_counts.get('é—´éš”è¿‡å°', 0) + 1
                    if diff_hours <= 0:
                        overlaps.append({
                            'position': i,
                            'time': timestamps_sorted.iloc[i],
                            'overlap_hours': abs(diff_hours)
                        })
            elif timeframe == '1D':
                diff_days = diff.total_seconds() / (24 * 3600)
                if 0.9 <= diff_days <= 1.1:  # å…è®¸å°è¯¯å·®
                    interval_counts['æ­£å¸¸'] = interval_counts.get('æ­£å¸¸', 0) + 1
                elif diff_days > 1.1:
                    interval_counts['é—´éš”è¿‡å¤§'] = interval_counts.get('é—´éš”è¿‡å¤§', 0) + 1
                    gaps.append({
                        'position': i,
                        'time': timestamps_sorted.iloc[i],
                        'gap_days': diff_days,
                        'missing_periods': int(diff_days) - 1
                    })
                elif diff_days < 0.9:
                    interval_counts['é—´éš”è¿‡å°'] = interval_counts.get('é—´éš”è¿‡å°', 0) + 1
                    if diff_days <= 0:
                        overlaps.append({
                            'position': i,
                            'time': timestamps_sorted.iloc[i],
                            'overlap_days': abs(diff_days)
                        })
        
        # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
        logger.info(f"   é—´éš”ç»Ÿè®¡:")
        for interval_type, count in interval_counts.items():
            percentage = (count / len(time_diffs)) * 100
            logger.info(f"     {interval_type}: {count} ({percentage:.1f}%)")
        
        # æ£€æŸ¥æ•°æ®è¿ç»­æ€§
        logger.info(f"\nğŸ” è¿ç»­æ€§æ£€æŸ¥:")
        
        # æ£€æŸ¥é‡å¤æ—¶é—´æˆ³
        duplicate_times = df.index.duplicated()
        if duplicate_times.any():
            dup_count = duplicate_times.sum()
            logger.warning(f"   âš ï¸ å‘ç° {dup_count} ä¸ªé‡å¤æ—¶é—´æˆ³")
            dup_times = df.index[duplicate_times]
            for dup_time in dup_times[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                logger.warning(f"      é‡å¤: {dup_time}")
        else:
            logger.info(f"   âœ… æ— é‡å¤æ—¶é—´æˆ³")
        
        # æ£€æŸ¥æ—¶é—´é¡ºåº
        is_sorted = timestamps.is_monotonic_increasing
        logger.info(f"   æ—¶é—´é¡ºåº: {'âœ… é€’å¢' if is_sorted else 'âŒ éé€’å¢'}")
        
        # æ£€æŸ¥é—´éš”
        if gaps:
            logger.warning(f"   âš ï¸ å‘ç° {len(gaps)} ä¸ªæ—¶é—´é—´éš”")
            logger.warning(f"   æœ€å¤§é—´éš”è¯¦æƒ…:")
            for gap in sorted(gaps, key=lambda x: x.get('gap_hours', x.get('gap_days', 0)), reverse=True)[:3]:
                if timeframe == '4H':
                    logger.warning(f"      ä½ç½® {gap['position']}: {gap['time']} (é—´éš” {gap['gap_hours']:.1f}å°æ—¶, ç¼ºå¤± {gap['missing_periods']} ä¸ªå‘¨æœŸ)")
                else:
                    logger.warning(f"      ä½ç½® {gap['position']}: {gap['time']} (é—´éš” {gap['gap_days']:.1f}å¤©, ç¼ºå¤± {gap['missing_periods']} ä¸ªå‘¨æœŸ)")
        else:
            logger.info(f"   âœ… æ— æ—¶é—´é—´éš”")
        
        # æ£€æŸ¥é‡å 
        if overlaps:
            logger.warning(f"   âš ï¸ å‘ç° {len(overlaps)} ä¸ªæ—¶é—´é‡å ")
            for overlap in overlaps[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                if timeframe == '4H':
                    logger.warning(f"      ä½ç½® {overlap['position']}: {overlap['time']} (é‡å  {overlap['overlap_hours']:.1f}å°æ—¶)")
                else:
                    logger.warning(f"      ä½ç½® {overlap['position']}: {overlap['time']} (é‡å  {overlap['overlap_days']:.1f}å¤©)")
        else:
            logger.info(f"   âœ… æ— æ—¶é—´é‡å ")
        
        # è®¡ç®—è¿ç»­æ€§è¯„åˆ†
        total_intervals = len(time_diffs)
        normal_intervals = interval_counts.get('æ­£å¸¸', 0)
        continuity_score = (normal_intervals / total_intervals) * 100 if total_intervals > 0 else 0
        
        logger.info(f"\nğŸ“Š è¿ç»­æ€§è¯„åˆ†:")
        logger.info(f"   è¿ç»­æ€§å¾—åˆ†: {continuity_score:.1f}%")
        
        if continuity_score >= 95:
            logger.info(f"   è¯„çº§: âœ… ä¼˜ç§€ (æ•°æ®é«˜åº¦è¿ç»­)")
        elif continuity_score >= 85:
            logger.info(f"   è¯„çº§: âœ… è‰¯å¥½ (æ•°æ®åŸºæœ¬è¿ç»­)")
        elif continuity_score >= 70:
            logger.info(f"   è¯„çº§: âš ï¸ ä¸€èˆ¬ (æœ‰å°‘é‡é—´éš”)")
        else:
            logger.info(f"   è¯„çº§: âŒ è¾ƒå·® (é—´éš”è¾ƒå¤š)")
        
        # æ˜¾ç¤ºæ—¶é—´åˆ†å¸ƒ
        logger.info(f"\nğŸ“… æ—¶é—´åˆ†å¸ƒåˆ†æ:")
        logger.info(f"   å¼€å§‹æ—¶é—´: {timestamps_sorted.iloc[0]}")
        logger.info(f"   ç»“æŸæ—¶é—´: {timestamps_sorted.iloc[-1]}")
        logger.info(f"   æ€»æ—¶é—´è·¨åº¦: {timestamps_sorted.iloc[-1] - timestamps_sorted.iloc[0]}")
        
        if timeframe == '4H':
            expected_total_hours = (len(df) - 1) * 4
            actual_total_hours = (timestamps_sorted.iloc[-1] - timestamps_sorted.iloc[0]).total_seconds() / 3600
            logger.info(f"   é¢„æœŸæ€»æ—¶é•¿: {expected_total_hours} å°æ—¶")
            logger.info(f"   å®é™…æ€»æ—¶é•¿: {actual_total_hours:.1f} å°æ—¶")
            logger.info(f"   æ—¶é•¿åŒ¹é…åº¦: {(expected_total_hours/actual_total_hours)*100:.1f}%")
        elif timeframe == '1D':
            expected_total_days = len(df) - 1
            actual_total_days = (timestamps_sorted.iloc[-1] - timestamps_sorted.iloc[0]).days
            logger.info(f"   é¢„æœŸæ€»å¤©æ•°: {expected_total_days} å¤©")
            logger.info(f"   å®é™…æ€»å¤©æ•°: {actual_total_days} å¤©")
            logger.info(f"   æ—¶é•¿åŒ¹é…åº¦: {(expected_total_days/actual_total_days)*100:.1f}%")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ” Kçº¿æ•°æ®è¿ç»­æ€§åˆ†æå·¥å…·")
    logger.info("=" * 60)
    
    # åˆ†æ4Hæ•°æ®
    data_4h_file = "./Kronos/finetune/data/prediction_data_4h.pkl"
    if os.path.exists(data_4h_file):
        logger.info("\nğŸ“Š åˆ†æ4Hæ•°æ®:")
        analyze_kline_continuity(data_4h_file, "4H")
    else:
        logger.warning("âš ï¸ 4Hæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
    
    # åˆ†æ1Dæ•°æ®
    data_1d_file = "./Kronos/finetune/data/prediction_data_1d.pkl"
    if os.path.exists(data_1d_file):
        logger.info("\nğŸ“Š åˆ†æ1Dæ•°æ®:")
        analyze_kline_continuity(data_1d_file, "1D")
    else:
        logger.warning("âš ï¸ 1Dæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
    
    logger.info("\nâœ… è¿ç»­æ€§åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()